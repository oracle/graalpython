diff --git a/c10/cuda/CUDACachingAllocator.cpp b/c10/cuda/CUDACachingAllocator.cpp
index 11bea6056..ca182f4ed 100644
--- a/c10/cuda/CUDACachingAllocator.cpp
+++ b/c10/cuda/CUDACachingAllocator.cpp
@@ -924,6 +924,8 @@ class DeviceCachingAllocator {
   // XXX - maybe we should generalize and have multiple events
   std::vector<OutOfMemoryObserver> oom_observers_;
 
+  std::vector<OutOfMemoryRetrier> oom_retriers_;
+
   std::vector<AllocatorTraceTracker> trace_trackers_;
 
   // mapping from block to a stream_set, containing streams on which the block
@@ -995,6 +997,10 @@ class DeviceCachingAllocator {
     oom_observers_.emplace_back(std::move(observer));
   }
 
+  void attachOutOfMemoryRetrier(OutOfMemoryRetrier retrier) {
+    oom_retriers_.emplace_back(std::move(retrier));
+  }
+
   void attachAllocatorTraceTracker(AllocatorTraceTracker tracker) {
     std::unique_lock<std::recursive_mutex> lock(mutex);
     trace_trackers_.emplace_back(std::move(tracker));
@@ -1019,6 +1025,9 @@ class DeviceCachingAllocator {
     // to have...
     auto context = maybeGatherContext(RecordContext::STATE);
 
+    int retries = 10;
+retry:
+
     std::unique_lock<std::recursive_mutex> lock(mutex);
 
     if (C10_LIKELY(captures_underway.empty())) {
@@ -1072,6 +1081,13 @@ class DeviceCachingAllocator {
     }
 
     if (!block_found) {
+      if (retries && !oom_retriers_.empty()) {
+          retries -= 1;
+          for (const auto& retrier : oom_retriers_) {
+              retrier();
+          }
+          goto retry;
+      }
       // For any error code other than cudaErrorMemoryAllocation,
       // alloc_block should have thrown an exception already.
       TORCH_INTERNAL_ASSERT(params.err == cudaErrorMemoryAllocation);
@@ -3046,6 +3062,12 @@ class NativeCachingAllocator : public CUDAAllocator {
     }
   }
 
+  void attachOutOfMemoryRetrier(OutOfMemoryRetrier retrier) override {
+    for (auto& allocator : device_allocator) {
+      allocator->attachOutOfMemoryRetrier(retrier);
+    }
+  }
+
   void attachAllocatorTraceTracker(AllocatorTraceTracker tracker) override {
     for (auto& allocator : device_allocator) {
       allocator->attachAllocatorTraceTracker(tracker);
diff --git a/c10/cuda/CUDACachingAllocator.h b/c10/cuda/CUDACachingAllocator.h
index 438ed8d77..a76348e2f 100644
--- a/c10/cuda/CUDACachingAllocator.h
+++ b/c10/cuda/CUDACachingAllocator.h
@@ -241,6 +241,8 @@ using OutOfMemoryObserver = std::function<void(
 
 using AllocatorTraceTracker = std::function<void(const TraceEntry&)>;
 
+using OutOfMemoryRetrier = std::function<void()>;
+
 class CUDAAllocator : public Allocator {
  public:
   virtual void* raw_alloc(size_t nbytes) = 0;
@@ -290,6 +292,7 @@ class CUDAAllocator : public Allocator {
       size_t alloc_trace_max_entries,
       RecordContext when) = 0;
   virtual void attachOutOfMemoryObserver(OutOfMemoryObserver observer) = 0;
+  virtual void attachOutOfMemoryRetrier(OutOfMemoryRetrier retrier) {};
 
   // Attached AllocatorTraceTracker callbacks will be called while the
   // per-device allocator lock is held. Any additional locks taken from within
@@ -444,6 +447,10 @@ inline void attachOutOfMemoryObserver(OutOfMemoryObserver observer) {
   return get()->attachOutOfMemoryObserver(std::move(observer));
 }
 
+inline void attachOutOfMemoryRetrier(OutOfMemoryRetrier retrier) {
+  return get()->attachOutOfMemoryRetrier(std::move(retrier));
+}
+
 inline void attachAllocatorTraceTracker(AllocatorTraceTracker tracker) {
   return get()->attachAllocatorTraceTracker(std::move(tracker));
 }
diff --git a/functorch/csrc/dim/dim.cpp b/functorch/csrc/dim/dim.cpp
index 252cb3b14..2b71b93eb 100644
--- a/functorch/csrc/dim/dim.cpp
+++ b/functorch/csrc/dim/dim.cpp
@@ -22,7 +22,10 @@ PyObject* Dim_init() {
 
 #include "minpybind.h"
 #include <frameobject.h>
+// GraalPy change
+#if 0 // GraalPy change
 #include <opcode.h>
+#endif // GraalPy change
 #include <utility>
 #include <new>
 #include <iostream>
@@ -37,11 +40,13 @@ PyObject* Dim_init() {
 #include "dim.h"
 #include "python_variable_simple.h"
 
+#if 0 // GraalPy change
 #if IS_PYTHON_3_11_PLUS
 #define Py_BUILD_CORE
 #include "internal/pycore_opcode.h"
 #undef Py_BUILD_CORE
 #endif
+#endif // GraalPy change
 
 // C++ API functions for objects to
 // * construct the object, returning a ref-counted handle
@@ -1468,6 +1473,7 @@ PyTypeObject Tensor::Type = {
 
 // dim() --------------------
 
+#if 0 // GraalPy change
 static bool relevant_op(_Py_CODEUNIT c) {
     switch(c) {
         case STORE_NAME:
@@ -1479,6 +1485,7 @@ static bool relevant_op(_Py_CODEUNIT c) {
             return false;
     }
 }
+#endif // GraalPy change
 
 static mpy::object create_dim(mpy::object name, mpy::handle size) {
     auto d = Dim::create(std::move(name));
@@ -1512,6 +1519,7 @@ static mpy::object create_dimlist(mpy::object name, mpy::handle size) {
 #endif
 
 namespace{
+#if 0 // GraalPy change
 struct PyInstDecoder {
     PyInstDecoder(PyCodeObject* code_object, int lasti)
     : code_object_(code_object), code_(_PyCode_CODE(code_object)), offset_(lasti / sizeof(_Py_CODEUNIT))  {}
@@ -1557,6 +1565,7 @@ private:
     _Py_CODEUNIT* code_;
     int offset_;
 };
+#endif // GraalPy change
 
 template<mpy::object (*create_object)(mpy::object, mpy::handle)>
 static PyObject* _dims(PyObject *self,
@@ -1582,6 +1591,7 @@ static PyObject* _dims(PyObject *self,
         }
     }
 
+#if 0 // GraalPy change
     PyThreadState* state = PyThreadState_GET();
     auto f = mpy::obj<PyFrameObject>::steal(PyThreadState_GetFrame(state));
     auto c = mpy::obj<PyCodeObject>::steal(PyFrame_GetCode(f.ptr()));
@@ -1602,10 +1612,12 @@ static PyObject* _dims(PyObject *self,
         found_ndims = decoder.oparg();
         decoder.next();
     }
+#endif // GraalPy change
 
     if (specified_ndims == -1) {
         if (found_ndims == 0) {
-            mpy::raise_error(PyExc_SyntaxError, "dims() must be assigned to a sequence of variable names or have argument n specified");
+            // GraalPy change
+            mpy::raise_error(PyExc_SyntaxError, "dims() without arguments doesn't work on GraalPy, use the explicit dims(number) form");
         }
         specified_ndims = found_ndims;
     }
@@ -1615,14 +1627,18 @@ static PyObject* _dims(PyObject *self,
 
     auto genobject = [&](int i) -> mpy::object {
         mpy::object name;
+#if 0 // GraalPy change
         if (i < found_ndims) {
             name = decoder.name();
         }
+#endif // GraalPy change
         if (!name.ptr()) {
             name = mpy::unicode_from_format("d%d", i);
             found_ndims = 0; // once we fail at finding a name, we can find any more
         } else {
+#if 0 // GraalPy change
             decoder.next();
+#endif // GraalPy change
         }
         return create_object(std::move(name), sizes != -1 ? mpy::sequence_view(py_sizes)[i] : mpy::handle(Py_None));
     };
@@ -2071,12 +2087,12 @@ struct IndexingInfo {
 IndexingInfo getsetitem_flat(Arena& A, TensorInfo self_info, Slice<mpy::handle> input, Slice<DimEntry> keys, Slice<mpy::handle> values, bool has_dimpacks_or_none);
 namespace{
 Slice<mpy::handle> as_slice(mpy::tuple_view tv) {
-    PyObject** begin = &PyTuple_GET_ITEM(tv.ptr(),0);
+    PyObject** begin = PySequence_Fast_ITEMS(tv.ptr());
     return Slice<mpy::handle>((mpy::handle*)begin, (mpy::handle*) (begin + tv.size()));
 }
 
 Slice<mpy::handle> as_slice(mpy::list_view tv) {
-    PyObject** begin = &PyList_GET_ITEM(tv.ptr(),0);
+    PyObject** begin = PySequence_Fast_ITEMS(tv.ptr());
     return Slice<mpy::handle>((mpy::handle*)begin, (mpy::handle*) (begin + tv.size()));
 }
 
diff --git a/pyproject.toml b/pyproject.toml
index 24a917b80..4887e9689 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -4,9 +4,11 @@ requires = [
     "wheel",
     "astunparse",
     "numpy",
-    "ninja",
+    # GraalPy change: require ninja on the system, the wheel wrapper goes through python, making the build very slow
+    # "ninja",
     "pyyaml",
-    "cmake",
+    # GraalPy change: same as ninja
+    # "cmake",
     "typing-extensions",
     "requests",
 ]
diff --git a/test/test_overrides.py b/test/test_overrides.py
index a55688b95..61bfad092 100644
--- a/test/test_overrides.py
+++ b/test/test_overrides.py
@@ -1543,12 +1543,9 @@ class TestTorchFunctionMode(TestCase):
             pass
 
         x = A(torch.randn(5))
-        with torch._C.DisableTorchFunctionSubclass():
-            g = torch._C._EnableTorchFunction()
-            try:
+        with torch._C.DisableTorchFunctionSubclass(), \
+            torch._C._EnableTorchFunction():
                 self.assertIsInstance(torch.sum(x), A)
-            finally:
-                del g
 
     def test_subclass_hash(self):
         class DiagTensor(torch.Tensor):
diff --git a/test/test_python_dispatch.py b/test/test_python_dispatch.py
index f5fdbf155..d76176cb6 100644
--- a/test/test_python_dispatch.py
+++ b/test/test_python_dispatch.py
@@ -2412,16 +2412,16 @@ def forward(self, x_1):
 class TestPythonDispatcher(TestCase):
     def test_basic(self):
         x = torch.randn(2, requires_grad=True)
-        r = torch._C._EnablePythonDispatcher()
-        torch.add(x, x)
+        with torch._C._EnablePythonDispatcher():
+            torch.add(x, x)
 
     def test_lstsq(self):
         a = torch.randn(4, 3)
         b = torch.rand(4, 3)
         expected_shape = torch.linalg.lstsq(a, b).solution.shape
-        r = torch._C._EnablePythonDispatcher()
-        python_disp_shape = torch.linalg.lstsq(a, b).solution.shape
-        self.assertEqual(expected_shape, python_disp_shape)
+        with torch._C._EnablePythonDispatcher():
+            python_disp_shape = torch.linalg.lstsq(a, b).solution.shape
+            self.assertEqual(expected_shape, python_disp_shape)
 
 
 class TestWrapperSubclassAliasing(TestCase):
diff --git a/third_party/fbgemm/CMakeLists.txt b/third_party/fbgemm/CMakeLists.txt
index 134523e7d..a00538e3c 100644
--- a/third_party/fbgemm/CMakeLists.txt
+++ b/third_party/fbgemm/CMakeLists.txt
@@ -10,6 +10,8 @@
 
 cmake_minimum_required(VERSION 3.16 FATAL_ERROR)
 
+add_compile_options(-Wno-error=maybe-uninitialized -Wno-error=uninitialized -Wno-error=restrict)
+
 list(APPEND CMAKE_MODULE_PATH "${CMAKE_CURRENT_SOURCE_DIR}/cmake/modules")
 
 # Define function to extract filelists from defs.bzl file
diff --git a/third_party/pybind11/include/pybind11/detail/common.h b/third_party/pybind11/include/pybind11/detail/common.h
index 454e6061b..7feafc7d7 100644
--- a/third_party/pybind11/include/pybind11/detail/common.h
+++ b/third_party/pybind11/include/pybind11/detail/common.h
@@ -300,7 +300,7 @@ PYBIND11_WARNING_DISABLE_MSVC(4505)
 #    define PYBIND11_INTERNAL_NUMPY_1_ONLY_DETECTED
 #endif
 
-#if defined(PYPY_VERSION) && !defined(PYBIND11_SIMPLE_GIL_MANAGEMENT)
+#if (defined(PYPY_VERSION) || defined(GRAALVM_PYTHON)) && !defined(PYBIND11_SIMPLE_GIL_MANAGEMENT)
 #    define PYBIND11_SIMPLE_GIL_MANAGEMENT
 #endif
 
diff --git a/third_party/pybind11/include/pybind11/detail/internals.h b/third_party/pybind11/include/pybind11/detail/internals.h
index c1047e4a0..c8453f30e 100644
--- a/third_party/pybind11/include/pybind11/detail/internals.h
+++ b/third_party/pybind11/include/pybind11/detail/internals.h
@@ -442,7 +442,7 @@ inline void translate_local_exception(std::exception_ptr p) {
 
 inline object get_python_state_dict() {
     object state_dict;
-#if PYBIND11_INTERNALS_VERSION <= 4 || PY_VERSION_HEX < 0x03080000 || defined(PYPY_VERSION)
+#if PYBIND11_INTERNALS_VERSION <= 4 || PY_VERSION_HEX < 0x03080000 || defined(PYPY_VERSION) || defined(GRAALVM_PYTHON)
     state_dict = reinterpret_borrow<object>(PyEval_GetBuiltins());
 #else
 #    if PY_VERSION_HEX < 0x03090000
diff --git a/third_party/pybind11/include/pybind11/detail/type_caster_base.h b/third_party/pybind11/include/pybind11/detail/type_caster_base.h
index 518d3107b..1b9edca02 100644
--- a/third_party/pybind11/include/pybind11/detail/type_caster_base.h
+++ b/third_party/pybind11/include/pybind11/detail/type_caster_base.h
@@ -519,7 +519,7 @@ PYBIND11_NOINLINE handle get_object_handle(const void *ptr, const detail::type_i
 }
 
 inline PyThreadState *get_thread_state_unchecked() {
-#if defined(PYPY_VERSION)
+#if defined(PYPY_VERSION) || defined(GRAALVM_PYTHON)
     return PyThreadState_GET();
 #elif PY_VERSION_HEX < 0x030D0000
     return _PyThreadState_UncheckedGet();
diff --git a/third_party/pybind11/include/pybind11/eval.h b/third_party/pybind11/include/pybind11/eval.h
index bd5f981f5..ee271672d 100644
--- a/third_party/pybind11/include/pybind11/eval.h
+++ b/third_party/pybind11/include/pybind11/eval.h
@@ -94,18 +94,18 @@ void exec(const char (&s)[N], object global = globals(), object local = object()
     eval<eval_statements>(s, std::move(global), std::move(local));
 }
 
-#if defined(PYPY_VERSION)
+#if defined(PYPY_VERSION) || defined(GRAALVM_PYTHON)
 template <eval_mode mode = eval_statements>
 object eval_file(str, object, object) {
-    pybind11_fail("eval_file not supported in PyPy3. Use eval");
+    pybind11_fail("eval_file not supported in PyPy3 or GraalPy. Use eval");
 }
 template <eval_mode mode = eval_statements>
 object eval_file(str, object) {
-    pybind11_fail("eval_file not supported in PyPy3. Use eval");
+    pybind11_fail("eval_file not supported in PyPy3 or GraalPy. Use eval");
 }
 template <eval_mode mode = eval_statements>
 object eval_file(str) {
-    pybind11_fail("eval_file not supported in PyPy3. Use eval");
+    pybind11_fail("eval_file not supported in PyPy3 or GraalPy. Use eval");
 }
 #else
 template <eval_mode mode = eval_statements>
diff --git a/third_party/pybind11/include/pybind11/pybind11.h b/third_party/pybind11/include/pybind11/pybind11.h
index 131d2e18a..86f3c5cad 100644
--- a/third_party/pybind11/include/pybind11/pybind11.h
+++ b/third_party/pybind11/include/pybind11/pybind11.h
@@ -591,8 +591,7 @@ protected:
                 // chain.
                 chain_start = rec;
                 rec->next = chain;
-                auto rec_capsule
-                    = reinterpret_borrow<capsule>(((PyCFunctionObject *) m_ptr)->m_self);
+                auto rec_capsule = reinterpret_borrow<capsule>(PyCFunction_GET_SELF(m_ptr));
                 rec_capsule.set_pointer(unique_rec.release());
                 guarded_strdup.release();
             } else {
@@ -652,9 +651,15 @@ protected:
 
         /* Install docstring */
         auto *func = (PyCFunctionObject *) m_ptr;
+#if !defined(GRAALVM_PYTHON)
         //std::free(const_cast<char *>(GraalPyCFunction_GetDoc((PyObject*)(func))));
         // Install docstring if it's non-empty (when at least one option is enabled)
         GraalPyCFunction_SetDoc((PyObject*)(func), signatures.empty() ? nullptr : PYBIND11_COMPAT_STRDUP(signatures.c_str()));
+#else
+        std::free(const_cast<char *>(GraalPyCFunction_GetDoc(m_ptr)));
+        GraalPyCFunction_SetDoc(
+            m_ptr, signatures.empty() ? nullptr : PYBIND11_COMPAT_STRDUP(signatures.c_str()));
+#endif
 
         if (rec->is_method) {
             m_ptr = PYBIND11_INSTANCE_METHOD_NEW(m_ptr, rec->scope.ptr());
@@ -2761,8 +2766,8 @@ get_type_override(const void *this_ptr, const type_info *this_type, const char *
     }
 
     /* Don't call dispatch code if invoked from overridden function.
-       Unfortunately this doesn't work on PyPy. */
-#if !defined(PYPY_VERSION)
+       Unfortunately this doesn't work on PyPy and GraalPy. */
+#if !defined(PYPY_VERSION) && !defined(GRAALVM_PYTHON)
 #    if PY_VERSION_HEX >= 0x03090000
     PyFrameObject *frame = PyThreadState_GetFrame(PyThreadState_Get());
     if (frame != nullptr) {
diff --git a/third_party/pybind11/include/pybind11/pytypes.h b/third_party/pybind11/include/pybind11/pytypes.h
index d777f31df..a446f4fb9 100644
--- a/third_party/pybind11/include/pybind11/pytypes.h
+++ b/third_party/pybind11/include/pybind11/pytypes.h
@@ -635,7 +635,7 @@ struct error_fetch_and_normalize {
 
         bool have_trace = false;
         if (m_trace) {
-#if !defined(PYPY_VERSION)
+#if !defined(PYPY_VERSION) && !defined(GRAALVM_PYTHON)
             auto *tb = reinterpret_cast<PyTracebackObject *>(m_trace.ptr());
 
             // Get the deepest trace possible.
diff --git a/tools/generate_torch_version.py b/tools/generate_torch_version.py
index 75ab4530e..8963ae58b 100644
--- a/tools/generate_torch_version.py
+++ b/tools/generate_torch_version.py
@@ -47,6 +47,7 @@ def get_tag(pytorch_root: Union[str, Path]) -> str:
 
 
 def get_torch_version(sha: Optional[str] = None) -> str:
+    return '2.4.1'
     pytorch_root = Path(__file__).parent.parent
     version = open(pytorch_root / "version.txt").read().strip()
 
diff --git a/torch/_dynamo/types.py b/torch/_dynamo/types.py
index 94b66e727..c4ba8280d 100644
--- a/torch/_dynamo/types.py
+++ b/torch/_dynamo/types.py
@@ -5,7 +5,8 @@ from typing import Any, Callable, Dict, List, NamedTuple, Optional, Protocol, Un
 from typing_extensions import TypeAlias
 
 
-if sys.version_info >= (3, 11):
+# GraalPy change
+if sys.version_info >= (3, 11) and not sys.implementation.name == 'graalpy':
     from torch._C._dynamo import eval_frame
 
     DynamoFrameType: TypeAlias = eval_frame._PyInterpreterFrame
diff --git a/torch/_tensor_str.py b/torch/_tensor_str.py
index 461f3a26b..bc529ca1b 100644
--- a/torch/_tensor_str.py
+++ b/torch/_tensor_str.py
@@ -693,6 +693,6 @@ def _functorch_wrapper_str_intern(tensor, *, tensor_contents=None):
 
 
 def _str(self, *, tensor_contents=None):
-    with torch.no_grad(), torch.utils._python_dispatch._disable_current_modes():
-        guard = torch._C._DisableFuncTorch()
+    with torch.no_grad(), torch.utils._python_dispatch._disable_current_modes(), \
+            torch._C._DisableFuncTorch():
         return _str_intern(self, tensor_contents=tensor_contents)
diff --git a/torch/csrc/Generator.cpp b/torch/csrc/Generator.cpp
index e94a0b198..696d7a919 100644
--- a/torch/csrc/Generator.cpp
+++ b/torch/csrc/Generator.cpp
@@ -291,7 +291,7 @@ PyObject* THPGenerator_reduce(PyObject* _self, PyObject* noargs) {
 static PyObject* THPGenerator_pickleSetState(PyObject* _self, PyObject* state) {
   HANDLE_TH_ERRORS
   THPGenerator_manualSeed(_self, PyTuple_GET_ITEM(state, 0));
-  auto& offset = PyTuple_GET_ITEM(state, 1);
+  PyObject* offset = PyTuple_GET_ITEM(state, 1);
   if (offset != Py_None) {
     THPGenerator_setOffset(_self, offset);
   }
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index da74bc9d3..4dfbc7316 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -429,46 +429,16 @@ PyObject* THPModule_addDocStr(PyObject* _unused, PyObject* args) {
     doc_str = all_docs.back().c_str();
   }
 
-  if (Py_TYPE(obj) == &PyCFunction_Type) {
-    PyCFunctionObject* f = (PyCFunctionObject*)obj;
-    if (GraalPyCFunction_GetDoc((PyObject*)(f))) {
-      return PyErr_Format(
-          PyExc_RuntimeError,
-          "function '%s' already has a docstring",
-          _PyCFunction_GetMethodDef((PyObject*)(f))->ml_name);
-    }
-    GraalPyCFunction_SetDoc((PyObject*)(f), doc_str);
-  } else if (strcmp(Py_TYPE(obj)->tp_name, "method_descriptor") == 0) {
-    PyMethodDescrObject* m = (PyMethodDescrObject*)obj;
-    if (m->d_method->ml_doc) {
-      return PyErr_Format(
-          PyExc_RuntimeError,
-          "method '%s' already has a docstring",
-          m->d_method->ml_name);
-    }
-    m->d_method->ml_doc = doc_str;
-  } else if (strcmp(Py_TYPE(obj)->tp_name, "getset_descriptor") == 0) {
-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-cstyle-cast)
-    PyGetSetDescrObject* m = (PyGetSetDescrObject*)obj;
-    if (m->d_getset->doc) {
-      return PyErr_Format(
-          PyExc_RuntimeError,
-          "attribute '%s' already has a docstring",
-          m->d_getset->name);
-    }
-    m->d_getset->doc = doc_str;
-  } else if (Py_TYPE(obj) == &PyType_Type) {
-    PyTypeObject* t = (PyTypeObject*)obj;
-    if (t->tp_doc) {
-      return PyErr_Format(
-          PyExc_RuntimeError, "Type '%s' already has a docstring", t->tp_name);
-    }
-    t->tp_doc = doc_str;
-  } else {
+  // GraalPy change
+  if (PyObject_GetDoc(obj)) {
     return PyErr_Format(
-        PyExc_TypeError,
-        "don't know how to add docstring to type '%s'",
-        Py_TYPE(obj)->tp_name);
+        PyExc_RuntimeError,
+        "object '%100R' already has a docstring",
+        obj);
+  }
+  // GraalPy change
+  if (PyObject_SetDoc(obj, doc_str) < 0) {
+      return NULL;
   }
 
   Py_INCREF(obj);
diff --git a/torch/csrc/autograd/python_variable_indexing.cpp b/torch/csrc/autograd/python_variable_indexing.cpp
index fdcafd6cd..1a90ff81a 100644
--- a/torch/csrc/autograd/python_variable_indexing.cpp
+++ b/torch/csrc/autograd/python_variable_indexing.cpp
@@ -144,25 +144,28 @@ inline Variable valueToTensor(
 
 static inline void recordSliceTrace(PyObject* obj) {
   PySliceObject* sliceobj = (PySliceObject*)obj;
-  if (THPVariable_Check(sliceobj->start)) {
+  PyObject* slicestart = PySlice_Start(sliceobj);
+  if (THPVariable_Check(slicestart)) {
     torch::jit::tracer::ArgumentStash::stashValue(
         std::string("start"),
         1,
-        THPVariable_Unpack(sliceobj->start),
+        THPVariable_Unpack(slicestart),
         torch::jit::IntType::get());
   }
-  if (THPVariable_Check(sliceobj->stop)) {
+  PyObject* slicestop = PySlice_Stop(sliceobj);
+  if (THPVariable_Check(slicestop)) {
     torch::jit::tracer::ArgumentStash::stashValue(
         std::string("end"),
         1,
-        THPVariable_Unpack(sliceobj->stop),
+        THPVariable_Unpack(slicestop),
         torch::jit::IntType::get());
   }
-  if (THPVariable_Check(sliceobj->step)) {
+  PyObject* slicestep = PySlice_Step(sliceobj);
+  if (THPVariable_Check(slicestep)) {
     torch::jit::tracer::ArgumentStash::stashValue(
         std::string("step"),
         1,
-        THPVariable_Unpack(sliceobj->step),
+        THPVariable_Unpack(slicestep),
         torch::jit::IntType::get());
   }
 }
diff --git a/torch/csrc/autograd/python_variable_indexing.h b/torch/csrc/autograd/python_variable_indexing.h
index 78c4a546d..182ad0b47 100644
--- a/torch/csrc/autograd/python_variable_indexing.h
+++ b/torch/csrc/autograd/python_variable_indexing.h
@@ -37,15 +37,16 @@ inline UnpackedSlice __PySlice_Unpack(PyObject* _r) {
     return val;
   };
 
-  if (r->step == Py_None) {
+  PyObject* stepObj = PySlice_Step(r);
+  if (stepObj == Py_None) {
     step_sym = c10::SymInt(1);
   } else {
-    if (torch::is_symint(r->step)) {
-      auto step_sym = py::handle(r->step).cast<c10::SymInt>();
+    if (torch::is_symint(stepObj)) {
+      auto step_sym = py::handle(stepObj).cast<c10::SymInt>();
     } else {
       // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
       Py_ssize_t step;
-      if (!_PyEval_SliceIndex(r->step, &step)) {
+      if (!_PyEval_SliceIndex(stepObj, &step)) {
         throw python_error();
       }
       if (step == 0) {
@@ -57,29 +58,31 @@ inline UnpackedSlice __PySlice_Unpack(PyObject* _r) {
     }
   }
 
-  if (torch::is_symint(r->start)) {
-    start_sym = py::handle(r->start).cast<c10::SymInt>();
-  } else if (r->start == Py_None) {
+  PyObject* startObj = PySlice_Start(r);
+  if (torch::is_symint(startObj)) {
+    start_sym = py::handle(startObj).cast<c10::SymInt>();
+  } else if (startObj == Py_None) {
     start_sym = c10::SymInt(step_sym < 0 ? PY_SSIZE_T_MAX : 0);
   } else {
     // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
     Py_ssize_t start;
-    if (!_PyEval_SliceIndex(r->start, &start)) {
+    if (!_PyEval_SliceIndex(startObj, &start)) {
       throw python_error();
     }
     start = clip_val(start);
     start_sym = c10::SymInt(start);
   }
 
-  if (torch::is_symint(r->stop)) {
-    stop_sym = py::handle(r->stop).cast<c10::SymInt>();
-  } else if (r->stop == Py_None) {
+  PyObject* stopObj = PySlice_Stop(r);
+  if (torch::is_symint(stopObj)) {
+    stop_sym = py::handle(stopObj).cast<c10::SymInt>();
+  } else if (stopObj == Py_None) {
     stop_sym = c10::SymInt(
         step_sym < 0 ? c10::SymInt::min_representable_int() : PY_SSIZE_T_MAX);
   } else {
     // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
     Py_ssize_t stop;
-    if (!_PyEval_SliceIndex(r->stop, &stop)) {
+    if (!_PyEval_SliceIndex(stopObj, &stop)) {
       throw python_error();
     }
     stop = clip_val(stop);
diff --git a/torch/csrc/cuda/Module.cpp b/torch/csrc/cuda/Module.cpp
index 4197c2aa5..d78e60b2b 100644
--- a/torch/csrc/cuda/Module.cpp
+++ b/torch/csrc/cuda/Module.cpp
@@ -1343,6 +1343,17 @@ static PyObject* THCPModule_initExtension(PyObject* self, PyObject* noargs) {
   poison_fork();
   at::globalContext().lazyInitCUDA();
 
+  // GraalPy change
+  auto retrier = [](){
+      py::gil_scoped_acquire g;
+      PyObject* gcmodule = PyImport_ImportModule("gc");
+      if (gcmodule) {
+          PyObject_CallMethod(gcmodule, "collect", NULL);
+      }
+      PyErr_Clear();
+  };
+  c10::cuda::CUDACachingAllocator::attachOutOfMemoryRetrier(std::move(retrier));
+
   auto m = THPObjectPtr(PyImport_ImportModule("torch.cuda"));
   if (!m)
     throw python_error();
diff --git a/torch/csrc/dynamo/cpython_defs.c b/torch/csrc/dynamo/cpython_defs.c
index c301da982..a2668be20 100644
--- a/torch/csrc/dynamo/cpython_defs.c
+++ b/torch/csrc/dynamo/cpython_defs.c
@@ -1,4 +1,5 @@
 #include <torch/csrc/dynamo/cpython_defs.h>
+#if 0 // GraalPy change
 
 #ifdef _WIN32
 #define unlikely(x) (x)
@@ -689,4 +690,10 @@ const int THP_PyOpcode_Caches_size = 0;
 
 #endif
 
-#endif // CPython 3.13
\ No newline at end of file
+#endif // CPython 3.13
+#else // GraalPy change
+
+const uint8_t* THP_PyOpcode_Caches = NULL;
+const int THP_PyOpcode_Caches_size = 0;
+
+#endif // GraalPy change
diff --git a/torch/csrc/dynamo/eval_frame.c b/torch/csrc/dynamo/eval_frame.c
index cbe9ab37a..18740a0d8 100644
--- a/torch/csrc/dynamo/eval_frame.c
+++ b/torch/csrc/dynamo/eval_frame.c
@@ -5,7 +5,9 @@
 #include <torch/csrc/dynamo/debug_macros.h>
 #include <torch/csrc/dynamo/extra_state.h>
 #include <torch/csrc/utils/python_compat.h>
+#if 0 // GraalPy change
 #include <opcode.h>
+#endif // GraalPy change
 #include <stdbool.h>
 
 
@@ -40,6 +42,7 @@ inline static void eval_frame_callback_set(PyObject* obj) {
 #undef _PyGC_FINALIZED
 #endif
 
+#if 0 // GraalPy change
 // see https://bugs.python.org/issue35886
 #if PY_VERSION_HEX >= 0x03080000
 #define Py_BUILD_CORE
@@ -722,6 +725,7 @@ static PyObject* decrement_working_threads(PyThreadState* tstate) {
   }
   Py_RETURN_NONE;
 }
+#endif // GraalPy change
 
 static PyObject* set_eval_frame(PyObject* new_callback, PyThreadState* tstate) {
   // Change the eval frame callback and return the old one
@@ -734,9 +738,13 @@ static PyObject* set_eval_frame(PyObject* new_callback, PyThreadState* tstate) {
   Py_INCREF(old_callback);
 
   if (old_callback != Py_None && new_callback == Py_None) {
-    decrement_working_threads(tstate);
+    // GraalPy change
+    PyErr_SetString(PyExc_NotImplementedError, "dynamo compilation is not supported on GraalPy");
+    return NULL;
   } else if (old_callback == Py_None && new_callback != Py_None) {
-    increment_working_threads(tstate);
+    // GraalPy change
+    PyErr_SetString(PyExc_NotImplementedError, "dynamo compilation is not supported on GraalPy");
+    return NULL;
   }
 
   Py_INCREF(new_callback);
@@ -825,12 +833,14 @@ static struct PyModuleDef _module = {
 #endif
 
 PyObject* torch_c_dynamo_eval_frame_init(void) {
+#if 0 // GraalPy change
   extra_index = _PyEval_RequestCodeExtraIndex(destroy_extra_state);
   if (extra_index < 0) {
     PyErr_SetString(PyExc_RuntimeError,
                     "dynamo: unable to register extra index");
     return NULL;
   }
+#endif // GraalPy change
 
   int result = PyThread_tss_create(&eval_frame_callback_key);
   CHECK(result == 0);
@@ -843,6 +853,7 @@ PyObject* torch_c_dynamo_eval_frame_init(void) {
     return NULL;
   }
 
+#if 0 // GraalPy change
 #if IS_PYTHON_3_11_PLUS
   if (PyType_Ready(&THPPyInterpreterFrameType) < 0) {
     return NULL;
@@ -852,6 +863,7 @@ PyObject* torch_c_dynamo_eval_frame_init(void) {
     return NULL;
   }
 #endif
+#endif // GraalPy change
 
   return module;
 }
diff --git a/torch/csrc/dynamo/extra_state.cpp b/torch/csrc/dynamo/extra_state.cpp
index 7c9b4be00..b8edbcfda 100644
--- a/torch/csrc/dynamo/extra_state.cpp
+++ b/torch/csrc/dynamo/extra_state.cpp
@@ -65,11 +65,13 @@ void destroy_extra_state(void* obj) {
 }
 
 void set_extra_state(PyCodeObject* code, ExtraState* extra_state) {
+#if 0 // GraalPy change
   ExtraState* old_extra_state = get_extra_state(code);
   CHECK(
       old_extra_state == nullptr || old_extra_state == SKIP_CODE ||
       old_extra_state != extra_state);
   _PyCode_SetExtra((PyObject*)code, extra_index, extra_state);
+#endif // GraalPy change
 }
 
 ExtraState* init_and_set_extra_state(PyCodeObject* code) {
diff --git a/torch/csrc/dynamo/guards.cpp b/torch/csrc/dynamo/guards.cpp
index b7fde50a9..712e3613b 100644
--- a/torch/csrc/dynamo/guards.cpp
+++ b/torch/csrc/dynamo/guards.cpp
@@ -26,7 +26,7 @@
 // https://github.com/python/cpython/blob/9afc6d102d16080535325f645849cd84eb04d57d/Objects/tupleobject.c#L1058-L1062
 // To handle this, we manually copy the struct here and manually cast it to this
 // new struct. From 3.12, the struct is included in the header file.
-#if IS_PYTHON_3_12_PLUS
+#if 0 // GraalPy change
 
 #define Py_BUILD_CORE
 // Bring _PyTupleIterObject from the header file
@@ -596,7 +596,7 @@ static PyObject* check_obj_id(PyObject* dummy, PyObject* args) {
   }
 }
 
-#if IS_PYTHON_3_12_PLUS
+#if 0 // GraalPy change
 
 static std::unordered_map<PyObject*, uint64_t> dict_version_map;
 static int dict_version_watcher_id;
@@ -617,7 +617,7 @@ static int dict_version_watch_callback(
 #endif
 
 static uint64_t get_dict_version_unchecked(PyObject* dict) {
-#if IS_PYTHON_3_12_PLUS
+#if 0 // GraalPy change
 
   if (PyDict_Watch(dict_version_watcher_id, dict)) {
     throw std::runtime_error("failed to add version watcher to dict!");
@@ -4013,7 +4013,7 @@ PyObject* torch_c_dynamo_guards_init() {
       "install_no_tensor_aliasing_guard", install_no_tensor_aliasing_guard);
 
 // initialize dict_version_map watcher for 3.12
-#if IS_PYTHON_3_12_PLUS
+#if 0 // GraalPy change
 
   dict_version_watcher_id = PyDict_AddWatcher(dict_version_watch_callback);
   if (dict_version_watcher_id == -1) {
diff --git a/torch/csrc/jit/python/python_tracer.cpp b/torch/csrc/jit/python/python_tracer.cpp
index 92e6e2d3a..4d2ec0bfe 100644
--- a/torch/csrc/jit/python/python_tracer.cpp
+++ b/torch/csrc/jit/python/python_tracer.cpp
@@ -31,11 +31,15 @@ std::vector<StackEntry> _pythonCallstack() {
   while (nullptr != frame) {
     auto code = THPCodeObjectPtr(PyFrame_GetCode(frame));
     size_t line = PyCode_Addr2Line(code.get(), PyFrame_GetLasti(frame));
-    std::string filename = THPUtils_unpackString(code->co_filename);
-    std::string funcname = THPUtils_unpackString(code->co_name);
+    PyObject* filenameObj = PyCode_GetFileName(code);
+    std::string filename = THPUtils_unpackString(filenameObj);
+    PyObject* funcnameObj = PyCode_GetName(code);
+    std::string funcname = THPUtils_unpackString(funcnameObj);
     auto source = std::make_shared<Source>(funcname, filename, line);
     entries.emplace_back(
         StackEntry{funcname, SourceRange(source, 0, funcname.size())});
+    Py_DECREF(funcnameObj);
+    Py_DECREF(filenameObj);
     auto new_frame = PyFrame_GetBack(frame);
     Py_DECREF(frame);
     frame = new_frame;
diff --git a/torch/csrc/profiler/python/combined_traceback.cpp b/torch/csrc/profiler/python/combined_traceback.cpp
index f9e20541e..f5d4d1375 100644
--- a/torch/csrc/profiler/python/combined_traceback.cpp
+++ b/torch/csrc/profiler/python/combined_traceback.cpp
@@ -86,8 +86,8 @@ struct PythonTraceback : public CapturedTraceback::Python {
     }
     for (const auto& f : to_symbolize) {
       auto f_code = (PyCodeObject*)f.code;
-      py::handle filename = f_code->co_filename;
-      py::handle funcname = f_code->co_name;
+      py::object filename = pybind11::reinterpret_steal<py::object>(PyCode_GetFileName(f_code));
+      py::object funcname = pybind11::reinterpret_steal<py::object>(PyCode_GetName(f_code));
       auto lineno = PyCode_Addr2Line(f_code, f.lasti);
       result.tracebacks.emplace_back();
       result.tracebacks.back().push_back(result.all_frames.size());
