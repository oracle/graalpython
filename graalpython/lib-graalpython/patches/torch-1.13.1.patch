diff --git a/CMakeLists.txt b/CMakeLists.txt
index e2e1f694..870ef417 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -155,7 +155,7 @@ endif()
 # Note to developers: if you add an option below, make sure you also add it to
 # cmake/Summary.cmake so that the summary prints out the option values.
 include(CMakeDependentOption)
-option(ATEN_NO_TEST "Do not build ATen test binaries" OFF)
+option(ATEN_NO_TEST "Do not build ATen test binaries" ON)
 option(BUILD_BINARY "Build C++ binaries" OFF)
 option(BUILD_DOCS "Build Caffe2 documentation" OFF)
 option(BUILD_CUSTOM_PROTOBUF "Build and use Caffe2's own protobuf under third_party" ON)
diff --git a/functorch/csrc/dim/dim.cpp b/functorch/csrc/dim/dim.cpp
index 9bab33b6..4f235d66 100644
--- a/functorch/csrc/dim/dim.cpp
+++ b/functorch/csrc/dim/dim.cpp
@@ -6,7 +6,7 @@
 
 #include "minpybind.h"
 #include <frameobject.h>
-#include <opcode.h>
+// #include <opcode.h>
 #include <utility>
 #include <new>
 #include <iostream>
@@ -1421,6 +1421,7 @@ PyTypeObject Tensor::Type = {
 
 // dim() --------------------
 
+/*
 bool relevant_op(_Py_CODEUNIT c) {
     switch(_Py_OPCODE(c)) {
         case STORE_NAME:
@@ -1459,6 +1460,7 @@ py::object getname(PyCodeObject* code, _Py_CODEUNIT c) {
     }
     return py::object::steal(PySequence_GetItem(names, _Py_OPARG(c)));
 }
+*/
 
 py::object create_dim(py::object name, py::handle size) {
     auto d = Dim::create(std::move(name));
@@ -1515,6 +1517,7 @@ static PyObject* _dims(PyObject *self,
         }
     }
 
+    /*
     PyThreadState* state = PyThreadState_GET();
     auto f = py::obj<PyFrameObject>::steal(PyThreadState_GetFrame(state));
     auto c = py::obj<PyCodeObject>::steal(PyFrame_GetCode(f.ptr()));
@@ -1532,10 +1535,12 @@ static PyObject* _dims(PyObject *self,
         found_ndims = _Py_OPARG(unpack);
         names_start++;
     }
+    */
 
     if (specified_ndims == -1) {
         if (found_ndims == 0) {
-            py::raise_error(PyExc_SyntaxError, "dims() must be assigned to a sequence of variable names or have argument n specified");
+            // GraalPy change
+            py::raise_error(PyExc_SyntaxError, "dims() without arguments doesn't work on GraalPy, use the explicit dims(number) form");
         }
         specified_ndims = found_ndims;
     }
@@ -1545,9 +1550,11 @@ static PyObject* _dims(PyObject *self,
 
     auto genobject = [&](int i) -> py::object {
         py::object name;
+        /*
         if (i < found_ndims) {
             name = getname(c.ptr(), code[names_start + i]);
         }
+        */
         if (!name.ptr()) {
             name = py::unicode_from_format("d%d", i);
             found_ndims = 0; // once we fail at finding a name, we can find any more
@@ -2004,12 +2011,12 @@ struct IndexingInfo {
 };
 
 static Slice<py::handle> as_slice(py::tuple_view tv) {
-    PyObject** begin = &PyTuple_GET_ITEM(tv.ptr(),0);
+    PyObject** begin = PySequence_Fast_ITEMS(tv.ptr());
     return Slice<py::handle>((py::handle*)begin, (py::handle*) (begin + tv.size()));
 }
 
 static Slice<py::handle> as_slice(py::list_view tv) {
-    PyObject** begin = &PyList_GET_ITEM(tv.ptr(),0);
+    PyObject** begin = PySequence_Fast_ITEMS(tv.ptr());
     return Slice<py::handle>((py::handle*)begin, (py::handle*) (begin + tv.size()));
 }
 
diff --git a/pyproject.toml b/pyproject.toml
index 827c03b8..36893a76 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -3,7 +3,7 @@ requires = [
     "setuptools",
     "wheel",
     "astunparse",
-    "numpy",
+    "numpy<2",
     "ninja",
     "pyyaml",
     "setuptools",
diff --git a/setup.py b/setup.py
index 2ef8b7f2..1f4efd70 100644
--- a/setup.py
+++ b/setup.py
@@ -244,6 +244,8 @@ from tools.generate_torch_version import get_torch_version
 # Parameters parsed from environment
 ################################################################################
 
+os.environ['BUILD_TEST'] = os.environ.get('BUILD_TEST', '0')
+
 VERBOSE_SCRIPT = True
 RUN_BUILD_DEPS = True
 # see if the user passed a quiet flag to setup.py arguments and respect
@@ -312,7 +314,8 @@ cmake_python_include_dir = sysconfig.get_path("include")
 ################################################################################
 package_name = os.getenv('TORCH_PACKAGE_NAME', 'torch')
 package_type = os.getenv('PACKAGE_TYPE', 'wheel')
-version = get_torch_version()
+#version = get_torch_version()
+version = '1.13.1'
 report("Building wheel {}-{}".format(package_name, version))
 
 cmake = CMake()
diff --git a/test/test_overrides.py b/test/test_overrides.py
index e9e01684..f4069bc9 100644
--- a/test/test_overrides.py
+++ b/test/test_overrides.py
@@ -1456,12 +1456,9 @@ class TestTorchFunctionMode(TestCase):
             pass
 
         x = A(torch.randn(5))
-        with torch._C.DisableTorchFunction():
-            g = torch._C._EnableTorchFunction()
-            try:
+        with torch._C.DisableTorchFunction(), \
+            torch._C._EnableTorchFunction():
                 self.assertIsInstance(torch.sum(x), A)
-            finally:
-                del g
 
     def test_subclass_hash(self):
         class DiagTensor(torch.Tensor):
diff --git a/test/test_python_dispatch.py b/test/test_python_dispatch.py
index dea96d19..4fb18b9e 100644
--- a/test/test_python_dispatch.py
+++ b/test/test_python_dispatch.py
@@ -1671,16 +1671,16 @@ $0 = torch._ops.aten.empty.memory_format([], device=device(type='cpu'), pin_memo
 class TestPythonDispatcher(TestCase):
     def test_basic(self):
         x = torch.randn(2, requires_grad=True)
-        r = torch._C._EnablePythonDispatcher()
-        torch.add(x, x)
+        with torch._C._EnablePythonDispatcher():
+            torch.add(x, x)
 
     def test_lstsq(self):
         a = torch.randn(4, 3)
         b = torch.rand(4, 3)
         expected_shape = torch.linalg.lstsq(a, b).solution.shape
-        r = torch._C._EnablePythonDispatcher()
-        python_disp_shape = torch.linalg.lstsq(a, b).solution.shape
-        self.assertEqual(expected_shape, python_disp_shape)
+        with torch._C._EnablePythonDispatcher():
+            python_disp_shape = torch.linalg.lstsq(a, b).solution.shape
+            self.assertEqual(expected_shape, python_disp_shape)
 
 if __name__ == '__main__':
     run_tests()
diff --git a/third_party/fbgemm/CMakeLists.txt b/third_party/fbgemm/CMakeLists.txt
index 58dcb9ae..b0ad68aa 100644
--- a/third_party/fbgemm/CMakeLists.txt
+++ b/third_party/fbgemm/CMakeLists.txt
@@ -144,7 +144,7 @@ if(MSVC)
 else(MSVC)
   string(APPEND CMAKE_CXX_FLAGS " -Wall")
   string(APPEND CMAKE_CXX_FLAGS " -Wextra")
-  string(APPEND CMAKE_CXX_FLAGS " -Werror")
+  #string(APPEND CMAKE_CXX_FLAGS " -Werror")
   string(APPEND CMAKE_CXX_FLAGS " -Wno-deprecated-declarations")
   target_compile_options(fbgemm_avx2 PRIVATE
     "-m64" "-mavx2" "-mf16c" "-mfma")
diff --git a/third_party/gloo/gloo/common/linux.cc b/third_party/gloo/gloo/common/linux.cc
index c5e6c887..423de67b 100644
--- a/third_party/gloo/gloo/common/linux.cc
+++ b/third_party/gloo/gloo/common/linux.cc
@@ -193,8 +193,8 @@ static int getInterfaceSpeedGLinkSettings(int sock, struct ifreq* ifr) {
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(4,6,0)
   constexpr auto link_mode_data_nwords = 3 * 127;
   struct {
-    struct ethtool_link_settings req;
     __u32 link_mode_data[link_mode_data_nwords];
+    struct ethtool_link_settings req;
   } ecmd;
   int rv;
 
diff --git a/third_party/gloo/gloo/transport/pair.h b/third_party/gloo/gloo/transport/pair.h
index e14d9cec..6de25a42 100644
--- a/third_party/gloo/gloo/transport/pair.h
+++ b/third_party/gloo/gloo/transport/pair.h
@@ -8,6 +8,7 @@
 
 #pragma once
 
+#include <array>
 #include <memory>
 
 #include "gloo/common/logging.h"
diff --git a/third_party/gloo/gloo/transport/tcp/device.h b/third_party/gloo/gloo/transport/tcp/device.h
index 6abbceab..ef0ffcab 100644
--- a/third_party/gloo/gloo/transport/tcp/device.h
+++ b/third_party/gloo/gloo/transport/tcp/device.h
@@ -8,6 +8,7 @@
 
 #pragma once
 
+#include <array>
 #include <atomic>
 #include <condition_variable>
 #include <memory>
diff --git a/third_party/pybind11/include/pybind11/gil.h b/third_party/pybind11/include/pybind11/gil.h
index a0b5de15..5a1f0c64 100644
--- a/third_party/pybind11/include/pybind11/gil.h
+++ b/third_party/pybind11/include/pybind11/gil.h
@@ -21,7 +21,7 @@ PyThreadState *get_thread_state_unchecked();
 
 PYBIND11_NAMESPACE_END(detail)
 
-#if defined(WITH_THREAD) && !defined(PYPY_VERSION)
+#if defined(WITH_THREAD) && !defined(PYPY_VERSION) && !defined(GRAALVM_PYTHON)
 
 /* The functions below essentially reproduce the PyGILState_* API using a RAII
  * pattern, but there are a few important differences:
@@ -172,7 +172,7 @@ private:
     bool disassoc;
     bool active = true;
 };
-#elif defined(PYPY_VERSION)
+#elif defined(PYPY_VERSION) || defined(GRAALVM_PYTHON)
 class gil_scoped_acquire {
     PyGILState_STATE state;
 
diff --git a/third_party/pybind11/include/pybind11/pytypes.h b/third_party/pybind11/include/pybind11/pytypes.h
index db8f2401..bac3c0d9 100644
--- a/third_party/pybind11/include/pybind11/pytypes.h
+++ b/third_party/pybind11/include/pybind11/pytypes.h
@@ -492,7 +492,7 @@ struct error_fetch_and_normalize {
 
         bool have_trace = false;
         if (m_trace) {
-#if !defined(PYPY_VERSION)
+#if !defined(PYPY_VERSION) && !defined(GRAALVM_PYTHON)
             auto *tb = reinterpret_cast<PyTracebackObject *>(m_trace.ptr());
 
             // Get the deepest trace possible.
diff --git a/torch/_dispatch/python.py b/torch/_dispatch/python.py
index 95b7fa05..8d6039c6 100644
--- a/torch/_dispatch/python.py
+++ b/torch/_dispatch/python.py
@@ -3,18 +3,8 @@ from contextlib import contextmanager
 
 __all__ = ['enable_python_dispatcher', 'no_python_dispatcher']
 
-@contextmanager
 def no_python_dispatcher():
-    g = torch._C._DisablePythonDispatcher()
-    try:
-        yield
-    finally:
-        del g
+    return torch._C._DisablePythonDispatcher()
 
-@contextmanager
 def enable_python_dispatcher():
-    g = torch._C._EnablePythonDispatcher()
-    try:
-        yield
-    finally:
-        del g
+    return torch._C._EnablePythonDispatcher()
diff --git a/torch/_tensor_str.py b/torch/_tensor_str.py
index 986be67a..53b5126a 100644
--- a/torch/_tensor_str.py
+++ b/torch/_tensor_str.py
@@ -632,6 +632,6 @@ def _functorch_wrapper_str_intern(tensor, *, tensor_contents=None):
 
 
 def _str(self, *, tensor_contents=None):
-    with torch.no_grad():
-        guard = torch._C._DisableFuncTorch()
+    with torch.no_grad(), \
+            torch._C._DisableFuncTorch():
         return _str_intern(self, tensor_contents=tensor_contents)
diff --git a/torch/autograd/grad_mode.py b/torch/autograd/grad_mode.py
index b847129d..3219d1e1 100644
--- a/torch/autograd/grad_mode.py
+++ b/torch/autograd/grad_mode.py
@@ -292,9 +292,10 @@ class inference_mode(_DecoratorContextManager):
 
     def __enter__(self):
         self._inference_mode_raii_guard = torch._C._InferenceMode(self.mode)
+        self._inference_mode_raii_guard.__enter__()
 
     def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        del self._inference_mode_raii_guard
+        self._inference_mode_raii_guard.__exit__(exc_type, exc_value, traceback)
 
     def clone(self):
         return self.__class__(self.mode)
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index 81e98428..df857f31 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -257,50 +257,16 @@ PyObject* THPModule_addDocStr(PyObject* _unused, PyObject* args) {
     doc_str = all_docs.back().c_str();
   }
 
-  if (Py_TYPE(obj) == &PyCFunction_Type) {
-    PyCFunctionObject* f = (PyCFunctionObject*)obj;
-    if (GraalPyCFunction_GetDoc((PyObject*)(f))) {
-      return PyErr_Format(
-          PyExc_RuntimeError,
-          "function '%s' already has a docstring",
-          _PyCFunction_GetMethodDef((PyObject*)(f))->ml_name);
-    }
-    GraalPyCFunction_SetDoc((PyObject*)(f), doc_str);
-  } else if (strcmp(Py_TYPE(obj)->tp_name, "method_descriptor") == 0) {
-    PyMethodDescrObject* m = (PyMethodDescrObject*)obj;
-    if (m->d_method->ml_doc) {
-      return PyErr_Format(
-          PyExc_RuntimeError,
-          "method '%s' already has a docstring",
-          m->d_method->ml_name);
-    }
-    m->d_method->ml_doc = doc_str;
-  } else if (strcmp(Py_TYPE(obj)->tp_name, "getset_descriptor") == 0) {
-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-cstyle-cast)
-    PyGetSetDescrObject* m = (PyGetSetDescrObject*)obj;
-    if (m->d_getset->doc) {
-      // NOLINTNEXTLINE(cppcoreguidelines-pro-type-vararg)
-      return PyErr_Format(
-          PyExc_RuntimeError,
-          "attribute '%s' already has a docstring",
-          m->d_getset->name);
-    }
-    // This field is not const for python < 3.7 yet the content is
-    // never modified.
-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)
-    m->d_getset->doc = const_cast<char*>(doc_str);
-  } else if (Py_TYPE(obj) == &PyType_Type) {
-    PyTypeObject* t = (PyTypeObject*)obj;
-    if (t->tp_doc) {
-      return PyErr_Format(
-          PyExc_RuntimeError, "Type '%s' already has a docstring", t->tp_name);
-    }
-    t->tp_doc = doc_str;
-  } else {
+  // GraalPy change
+  if (PyObject_GetDoc(obj)) {
     return PyErr_Format(
-        PyExc_TypeError,
-        "don't know how to add docstring to type '%s'",
-        Py_TYPE(obj)->tp_name);
+        PyExc_RuntimeError,
+        "object '%100R' already has a docstring",
+        obj);
+  }
+  // GraalPy change
+  if (PyObject_SetDoc(obj, doc_str) < 0) {
+      return NULL;
   }
 
   Py_INCREF(obj);
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index a1d6de21..4a8e6487 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -43,27 +43,42 @@ struct DisableFuncTorch {
   c10::impl::ExcludeDispatchKeyGuard back_guard_;
 };
 
+struct DisableFuncTorchWrapper {
+  DisableFuncTorch* delegate = nullptr;
+};
+
 struct EnableTorchFunction {
   EnableTorchFunction()
       : old_(at::impl::PythonTorchFunctionTLS::is_disabled()) {
-    at::impl::PythonTorchFunctionTLS::set_disabled(false);
-  }
-  ~EnableTorchFunction() {
-    at::impl::PythonTorchFunctionTLS::set_disabled(old_);
   }
   bool old_;
 };
 
 struct EnablePythonDispatcher {
   EnablePythonDispatcher() : old_(c10::impl::PythonDispatcherTLS::get_state()) {
-    c10::impl::PythonDispatcherTLS::set_state(getPyInterpreter());
-  }
-  ~EnablePythonDispatcher() {
-    c10::impl::PythonDispatcherTLS::set_state(old_);
   }
   c10::impl::PyInterpreter* old_;
 };
 
+struct DisableTorchDispatchWrapper {
+  torch::DisableTorchDispatch* delegate = nullptr;
+};
+
+struct InferenceModeWrapper {
+  bool enabled;
+  torch::InferenceMode* delegate = nullptr;
+
+  InferenceModeWrapper(bool enabled) : enabled(enabled) {}
+};
+
+struct RestorePythonTLSSnapshotWrapper {
+  at::impl::RestorePythonTLSSnapshot* delegate = nullptr;
+};
+
+struct DisablePythonDispatcherWrapper {
+  c10::impl::DisablePythonDispatcher* delegate = nullptr;
+};
+
 } // namespace
 
 PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
@@ -337,23 +352,92 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
 
   _C_m.def("_activate_cuda_trace", []() { activateCUDATrace(); });
 
-  py::class_<c10::InferenceMode>(_C_m, "_InferenceMode").def(py::init<bool>());
+  py::class_<InferenceModeWrapper>(_C_m, "_InferenceMode")
+      .def(py::init<bool>())
+      .def("__enter__", [&] (InferenceModeWrapper& w) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+        w.delegate = new c10::InferenceMode(w.enabled);
+      })
+      .def("__exit__", [&] (InferenceModeWrapper& w, py::object& excType, py::object& excValue, py::object& excTb) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+      });
 
-  py::class_<at::impl::RestorePythonTLSSnapshot>(
+  py::class_<RestorePythonTLSSnapshotWrapper>(
       _C_m, "_RestorePythonTLSSnapshot")
-      .def(py::init<>());
+      .def(py::init<>())
+      .def("__enter__", [&] (RestorePythonTLSSnapshotWrapper& w) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+        w.delegate = new at::impl::RestorePythonTLSSnapshot;
+      })
+      .def("__exit__", [&] (RestorePythonTLSSnapshotWrapper& w, py::object& excType, py::object& excValue, py::object& excTb) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+      });
 
   // TODO: line up this binding with DisableTorchFunction
-  py::class_<torch::DisableTorchDispatch>(_C_m, "_DisableTorchDispatch")
-      .def(py::init<>());
+  py::class_<DisableTorchDispatchWrapper>(_C_m, "_DisableTorchDispatch")
+      .def(py::init<>())
+      .def("__enter__", [&] (DisableTorchDispatchWrapper& w) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+        w.delegate = new torch::DisableTorchDispatch;
+      })
+      .def("__exit__", [&] (DisableTorchDispatchWrapper& w, py::object& excType, py::object& excValue, py::object& excTb) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+      });
   py::class_<EnableTorchFunction>(_C_m, "_EnableTorchFunction")
-      .def(py::init<>());
+      .def(py::init<>())
+      .def("__enter__", [&] (EnableTorchFunction& w) {
+        at::impl::PythonTorchFunctionTLS::set_disabled(false);
+      })
+      .def("__exit__", [&] (EnableTorchFunction& w, py::object& excType, py::object& excValue, py::object& excTb) {
+        at::impl::PythonTorchFunctionTLS::set_disabled(w.old_);
+      });
   py::class_<EnablePythonDispatcher>(_C_m, "_EnablePythonDispatcher")
-      .def(py::init<>());
-  py::class_<c10::impl::DisablePythonDispatcher>(
+      .def(py::init<>())
+      .def("__enter__", [&] (EnablePythonDispatcher& w) {
+        c10::impl::PythonDispatcherTLS::set_state(getPyInterpreter());
+      })
+      .def("__exit__", [&] (EnablePythonDispatcher& w, py::object& excType, py::object& excValue, py::object& excTb) {
+        c10::impl::PythonDispatcherTLS::set_state(w.old_);
+      });
+  py::class_<DisablePythonDispatcherWrapper>(
       _C_m, "_DisablePythonDispatcher")
-      .def(py::init<>());
-  py::class_<DisableFuncTorch>(_C_m, "_DisableFuncTorch").def(py::init<>());
+      .def(py::init<>())
+      .def("__enter__", [&] (DisablePythonDispatcherWrapper& w) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+        w.delegate = new c10::impl::DisablePythonDispatcher;
+      })
+      .def("__exit__", [&] (DisablePythonDispatcherWrapper& w, py::object& excType, py::object& excValue, py::object& excTb) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+      });
+  py::class_<DisableFuncTorchWrapper>(_C_m, "_DisableFuncTorch")
+      .def(py::init<>())
+      .def("__enter__", [&] (DisableFuncTorchWrapper& w) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+        w.delegate = new DisableFuncTorch;
+      })
+      .def("__exit__", [&] (DisableFuncTorchWrapper& w, py::object& excType, py::object& excValue, py::object& excTb) {
+        if (w.delegate) {
+          delete w.delegate;
+        }
+      });
 
   py::class_<torch::autograd::SavedVariable>(m, "SavedTensor")
       .def(py::init([]() -> torch::autograd::SavedVariable {
diff --git a/torch/csrc/autograd/python_variable_indexing.cpp b/torch/csrc/autograd/python_variable_indexing.cpp
index 8c9ed1d7..69ecb4ca 100644
--- a/torch/csrc/autograd/python_variable_indexing.cpp
+++ b/torch/csrc/autograd/python_variable_indexing.cpp
@@ -142,25 +142,28 @@ static inline void checkUnpackSlice(
 
 static inline void recordSliceTrace(PyObject* obj) {
   PySliceObject* sliceobj = (PySliceObject*)obj;
-  if (THPVariable_Check(sliceobj->start)) {
+  PyObject* slicestart = PySlice_Start(sliceobj);
+  if (THPVariable_Check(slicestart)) {
     torch::jit::tracer::ArgumentStash::stashValue(
         std::string("start"),
         1,
-        THPVariable_Unpack(sliceobj->start),
+        THPVariable_Unpack(slicestart),
         torch::jit::IntType::get());
   }
-  if (THPVariable_Check(sliceobj->stop)) {
+  PyObject* slicestop = PySlice_Stop(sliceobj);
+  if (THPVariable_Check(slicestop)) {
     torch::jit::tracer::ArgumentStash::stashValue(
         std::string("end"),
         1,
-        THPVariable_Unpack(sliceobj->stop),
+        THPVariable_Unpack(slicestop),
         torch::jit::IntType::get());
   }
-  if (THPVariable_Check(sliceobj->step)) {
+  PyObject* slicestep = PySlice_Step(sliceobj);
+  if (THPVariable_Check(slicestep)) {
     torch::jit::tracer::ArgumentStash::stashValue(
         std::string("step"),
         1,
-        THPVariable_Unpack(sliceobj->step),
+        THPVariable_Unpack(slicestep),
         torch::jit::IntType::get());
   }
 }
diff --git a/torch/csrc/jit/python/python_tracer.cpp b/torch/csrc/jit/python/python_tracer.cpp
index 78676e2e..e6a3caf0 100644
--- a/torch/csrc/jit/python/python_tracer.cpp
+++ b/torch/csrc/jit/python/python_tracer.cpp
@@ -33,11 +33,15 @@ std::vector<StackEntry> _pythonCallstack() {
   while (nullptr != frame) {
     auto code = THPCodeObjectPtr(PyFrame_GetCode(frame));
     size_t line = PyCode_Addr2Line(code.get(), PyFrame_GetLasti(frame));
-    std::string filename = THPUtils_unpackString(code->co_filename);
-    std::string funcname = THPUtils_unpackString(code->co_name);
+    PyObject* filenameObj = PyCode_GetFileName(code);
+    std::string filename = THPUtils_unpackString(filenameObj);
+    PyObject* funcnameObj = PyCode_GetName(code);
+    std::string funcname = THPUtils_unpackString(funcnameObj);
     auto source = std::make_shared<Source>(funcname, filename, line);
     entries.emplace_back(
         StackEntry{funcname, SourceRange(source, 0, funcname.size())});
+    Py_DECREF(funcnameObj);
+    Py_DECREF(filenameObj);
     auto new_frame = PyFrame_GetBack(frame);
     Py_DECREF(frame);
     frame = new_frame;
diff --git a/torch/csrc/utils/python_compat.h b/torch/csrc/utils/python_compat.h
index 44911527..d3ad9c54 100644
--- a/torch/csrc/utils/python_compat.h
+++ b/torch/csrc/utils/python_compat.h
@@ -96,7 +96,7 @@ static inline void _Py_SET_TYPE(PyObject* ob, PyTypeObject* type) {
 //#define Py_SET_TYPE(ob, type) _Py_SET_TYPE((PyObject*)(ob), type)
 #endif
 
-#if PY_VERSION_HEX < ((3 << 24) | (11 << 16) | (0 << 8) | (0xA << 4) | (4 << 0))
+#if (PY_VERSION_HEX < ((3 << 24) | (11 << 16) | (0 << 8) | (0xA << 4) | (4 << 0))) && !defined(GRAALVM_PYTHON)
 static inline PyObject* PyFrame_GetLocals(PyFrameObject* frame) {
   PyFrame_FastToLocals(frame);
   auto res = frame->f_locals;
diff --git a/torch/distributed/_shard/partial_tensor.py b/torch/distributed/_shard/partial_tensor.py
index dc8d09bd..d7e4ec6c 100644
--- a/torch/distributed/_shard/partial_tensor.py
+++ b/torch/distributed/_shard/partial_tensor.py
@@ -234,14 +234,11 @@ class _PartialTensor(torch.Tensor):
             return _PARTIAL_TENSOR_OPS[func](types, args, kwargs, process_group)
 
         # Need to disable all dispatch to print args and kwargs appropriately.
-        guard = torch._C._DisableTorchDispatch()  # type: ignore[attr-defined]
-        try:
-            with torch._C.DisableTorchFunction():
-                raise RuntimeError(
-                    f"torch function '{func.__name__}', with args: {args} and "
-                    f"kwargs: {kwargs} not supported for PartialTensor!")
-        finally:
-            del guard
+        with torch._C._DisableTorchDispatch(), \
+                torch._C.DisableTorchFunction():
+            raise RuntimeError(
+                f"torch function '{func.__name__}', with args: {args} and "
+                f"kwargs: {kwargs} not supported for PartialTensor!")
 
     @classmethod
     def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
diff --git a/torch/overrides.py b/torch/overrides.py
index dbee241b..dbc601e4 100644
--- a/torch/overrides.py
+++ b/torch/overrides.py
@@ -1885,9 +1885,10 @@ def _no_torch_function_mode() -> Iterator[None]:
 class enable_reentrant_dispatch():
     def __enter__(self):
         self._raii_guard = torch._C._RestorePythonTLSSnapshot()
+        self._raii_guard.__enter__()
 
     def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        del self._raii_guard
+        self._raii_guard.__exit__(exc_type, exc_value, traceback)
 
 def get_buffer(tensor_subclass, data, prefix):
     import ctypes
diff --git a/torch/testing/_internal/common_utils.py b/torch/testing/_internal/common_utils.py
index e3285090..7f9059aa 100644
--- a/torch/testing/_internal/common_utils.py
+++ b/torch/testing/_internal/common_utils.py
@@ -1347,13 +1347,8 @@ def set_rng_seed(seed):
         np.random.seed(seed)
 
 
-@contextmanager
 def disable_functorch():
-    guard = torch._C._DisableFuncTorch()  # type: ignore[attr-defined]
-    try:
-        yield
-    finally:
-        del guard
+    return torch._C._DisableFuncTorch()
 
 
 @contextlib.contextmanager
diff --git a/torch/utils/_mode_utils.py b/torch/utils/_mode_utils.py
index f9098c6d..adcd4920 100644
--- a/torch/utils/_mode_utils.py
+++ b/torch/utils/_mode_utils.py
@@ -8,10 +8,5 @@ T = TypeVar('T')
 def all_same_mode(modes):
     return all(tuple(mode == modes[0] for mode in modes))
 
-@contextmanager
 def no_dispatch():
-    guard = torch._C._DisableTorchDispatch()  # type: ignore[attr-defined]
-    try:
-        yield
-    finally:
-        del guard
+    return torch._C._DisableTorchDispatch()
